---
toc: true
tags: HPC MPI 
title: "[HPC]What is MPI (with CUDA)"
---

## MPI란?
* 병렬 프로그래밍을 위한 표준화된 데이터 통신 라이브러리이다.
* Message: Data+envelop
  * 메시지라는 것은 데이터를 봉투에 담아있는 상태로 볼 수 있다.
  * 이 메세지 안에는 데이터뿐만 아니라, (1) 어떤 프로세스가 보내고 받는지, (2) 어디에 데이터가 있는지, (3) 데이터의 크기는 얼마나 크고, 얼마나 공간을 확보해야되는지 등 프로세스간 통신하는데 필요한 정보를 담고 있다.
  * Tag(메시지의 고유번호): 메시지를 매칭하기 위해 다는 구분자이다. 순서대로 메시지 도착을 처리할 수 도 있고, 와일드 카드를 사용할 수도 있다.
  * 커뮤니케이터(Communicator) & 랭크(Rank): 커뮤니케이터란, 통신이 허용되는 프로세서들의 집합이다. 각 프로세서들을 식별하기위해서 Rank를 붙여준다. 

* 프로세스 기준으로 작업을 할당한다.
  * MPI를 통한 작업은, 프로세스들을 프로세서에게 부여하고 처리하고, 프로세서끼리 결과를 송수신하기를 반복하며 그 목적을 수행한다.
* MP(Message Passing)이란!?
  * 메시지 전달이란, 정보를 동일한 주소공간을 공유하지 않더라도, 프로세서 간 데이터를 주고받는 통신을 하므로 동기화된 작업을 할 수 있도록하는 방법이다. 
  * 그러니 컴퓨터들이 네트워크에 연결되어있는 상황과 같은 여러자원을 동시에 사용하는 상황에 유용하다. 
  
![img1 daumcdn](https://user-images.githubusercontent.com/67637935/153751880-7aaac248-e8ac-484e-bb37-58dfc1b3f966.png)

  * 어떤 프로세서에서 타 컴퓨터의 메모리에 직접 통신하는게 아니라, 메시지라는 간접적인 방법을 통해서 통신하는 것이다.
  * 프로세서 1에서 어떤 메시지를 보내면서 TAG를 걸어둔다.
  * 그리고 프로세서 2에서는 그 TAG는 내가 필요하니 가져가겠다고 여러 메시지들을 TAG로 분별하여 필요한 메세지를 가져가는 방식이다.
  * 직접 메모리를 공유하는 방식에 비해 비효율적이지만, 네트워크만 연결되어 있으면 여러 계산자원을 동시에 사용할 수 있다. 

## MPI Model
* 하나의 작업을 다수의 프로세스들에게 나누어 실행시키는 병렬 계산에서는 필연적으로 프로세스들 사이의 통신이 필요하다.
* 메시지 패싱 모델은 각자의 메모리를 지역적으로 따로 가지는 프로세스들로 구성된 분산 시스템 환경에서, 프로세스들 사이의 통신을 오직 메시지들의 송신(sending)과 수신(receiving)으로만 구현하는 프로그래밍 모델을 말한다.)
* 메시지 패싱 모델은 프로세스들이 메모리 공간을 공유하지 않으며 한 프로세스가 다른 프로세스에 직접 접근하는 것을 허용하지 않는다. 

![1-1](https://user-images.githubusercontent.com/67637935/153753887-851f0329-4c84-45e1-a649-6d1c9a8f569f.jpg)

![1-2](https://user-images.githubusercontent.com/67637935/153753918-e167aaaa-a47a-4cf5-ba3a-39b734ef0886.jpg)


## Usage of MPI Example

> ref: <https://wooono.tistory.com/49>

* 1부터 1000까지를 더하는 work가 있다고 하자, 만약 코어가 1개라면, 1개의 코어를 이용하여 1부터 100까지 더하면된다.
* 만약 코어가 10개이고 이 작업을 더 빠르게 수행하고 싶다면, 1부터 1000 사이를 10개의 구간으로 쪼개어,
* 첫번째 코어는 1부터 100까지 더하게 하고, 두번째 코어는 101부터 200까지 더하게 하여 10개의 코어에 작업을 분산시킨다.
* 그 후 10개의 코어의 결과물들을 한 군데 취합하여 10개의 숫자를 더하면 최종결과가 얻어진다.
* "이때 결과물들을 한군데 취합하여 최종결과를 얻는 과정"이 MPI가 담당하는 과정이다. 
* 즉, **알고리즘을 병렬화하고, 병렬화 과정에서 코어 사이의 정보교환 단계에 MPI가 사용된다.**

## Why we use MPI+CUDA

* data size가 너무 커서 하나의 GPU로 실행이 불가능한 경우 or  실행은 가능하나, 시간이 오래걸리는 경우
* GPU를 사용하여 기존 MPI 프로그램을 가속화시키고 싶은 경우




## How to do MPI Programming

> ref: <http://ap2.khu.ac.kr/download/mpi_lec.pdf>

